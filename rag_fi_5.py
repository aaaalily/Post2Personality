import pandas as pd
from openai import OpenAI
from tqdm import tqdm
import re
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score
from sentence_transformers import SentenceTransformer
import faiss
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import os

# é…ç½®å‚æ•°
BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
ADAPTER_PATH = "./results/checkpoint-2170" 
ENCODER_PATH = "all-MiniLM-L6-v2"
OPENAI_API_KEY = "YOUR_API_KEY"
OPENAI_BASE_URL = "https://api.deepseek.com"

class HybridMBTISystem:
    def __init__(self, adapter_path, encoder_path, vector_db_dir="./vector_db"):
        print("ğŸš€ åˆå§‹åŒ–æ··åˆåˆ†æç³»ç»Ÿ...")
        # åˆå§‹åŒ–å­˜å‚¨è·¯å¾„
        self.vector_db_dir = vector_db_dir
        os.makedirs(vector_db_dir, exist_ok=True)
        
        # å®šä¹‰æ–‡ä»¶è·¯å¾„
        self.index_path = os.path.join(vector_db_dir, "faiss_index.index")
        self.encoder_path = os.path.join(vector_db_dir, "sentence_encoder") 
        self.data_path = os.path.join(vector_db_dir, "retrieval_data.feather")
        
        # åŠ è½½ç»„ä»¶
        self._load_local_components(adapter_path)
        self.encoder = SentenceTransformer(encoder_path)
        self.index = None
        self.retrieval_df = None
        self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def _load_local_components(self, adapter_path):
        """åŠ è½½æœ¬åœ°æ¨¡å‹ç»„ä»¶"""
        print("ğŸ”§ åŠ è½½æœ¬åœ°æ¨¡å‹...")
        try:
            # åŠ è½½åŸºç¡€æ¨¡å‹é…ç½®
            peft_config = PeftConfig.from_pretrained(adapter_path)
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            self.base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # åŠ è½½é€‚é…å™¨
            self.model = PeftModel.from_pretrained(self.base_model, adapter_path)
            self.model.eval()
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise

    def build_vector_db(self, df, force_rebuild=False):
        """æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
        if not force_rebuild and self._try_load_vector_db():
            print("âœ… åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“")
            return
            
        print("ğŸ§  æ„å»ºæ–°å‘é‡æ•°æ®åº“...")
        tqdm.pandas()
        df['enhanced_vectors'] = df['posts'].progress_apply(
            lambda x: self._enhance_text_representation(x)
        )
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = len(df['enhanced_vectors'].iloc[0])
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(np.array(df['enhanced_vectors'].tolist()))
        self.retrieval_df = df
        
        # ä¿å­˜èµ„æº
        self._save_vector_db()
        print(f"ğŸ“Š æ•°æ®åº“æ„å»ºå®Œæˆï¼å…± {len(df)} æ¡å‘é‡")

    def _save_vector_db(self):
        """ä¿å­˜æ‰€æœ‰å¿…è¦æ–‡ä»¶"""
        print("ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“...")
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, self.index_path)
        
        # ä¿å­˜SentenceTransformerç¼–ç å™¨
        self.encoder.save(self.encoder_path)
        
        # ä¿å­˜æ£€ç´¢æ•°æ®ï¼ˆä½¿ç”¨featheræ ¼å¼æ›´é«˜æ•ˆï¼‰
        self.retrieval_df.to_feather(self.data_path)
        print(f"âœ… å·²ä¿å­˜è‡³ç›®å½•: {self.vector_db_dir}")

    def _try_load_vector_db(self):
        """å°è¯•åŠ è½½å·²æœ‰æ•°æ®åº“"""
        required_files = [self.index_path, self.encoder_path, self.data_path]
        if all(os.path.exists(p) for p in required_files):
            try:
                print("â³ å°è¯•åŠ è½½é¢„å­˜å‘é‡æ•°æ®åº“...")
                self.index = faiss.read_index(self.index_path)
                self.encoder = SentenceTransformer(self.encoder_path)
                self.retrieval_df = pd.read_feather(self.data_path)
                print("âœ… åŠ è½½æˆåŠŸï¼")
                return True
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥: {str(e)}")
                return False
        print("â„¹ï¸ æœªæ‰¾åˆ°é¢„å­˜æ•°æ®åº“æ–‡ä»¶")
        return False

    def _enhance_text_representation(self, text):
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¢å¼ºæ–‡æœ¬è¡¨ç¤º"""
        # è·å–åŸºç¡€embedding
        base_embed = self.encoder.encode(text)
        
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æå–é«˜çº§ç‰¹å¾
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
            outputs = self.model(**inputs, output_hidden_states=True)
            last_hidden = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()[0]
        
        # æ‹¼æ¥åŸºç¡€embeddingå’Œé«˜çº§ç‰¹å¾
        return np.concatenate([base_embed, last_hidden])

    def retrieve_enhanced(self, query, k=5):
        """å¢å¼ºæ£€ç´¢ï¼šç»“åˆè¯­ä¹‰å’Œæ¨¡å‹ç†è§£"""
        if self.index is None:
            raise ValueError("è¯·å…ˆæ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“ï¼")
        
        query_vec = self._enhance_text_representation(query)
        distances, indices = self.index.search(np.array([query_vec]), k)
        
        return [
            (self.retrieval_df.iloc[idx]['posts'],
             self.retrieval_df.iloc[idx]['type'])
            for idx in indices[0] if idx >= 0
        ]

    def generate_with_api(self, text, top_k=5):
        """ç”Ÿæˆé˜¶æ®µï¼šç»“åˆæ£€ç´¢ç»“æœä½¿ç”¨API"""
        try:
            # å¢å¼ºæ£€ç´¢ï¼ˆä½¿ç”¨top_kå‚æ•°ï¼‰
            similar_samples = self.retrieve_enhanced(text, k=top_k)
            
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆ†æå…³é”®ç‰¹å¾
            analysis = self._analyze_with_local_model(text)
            
            # æ„å»ºæ™ºèƒ½æç¤º
            prompt = self._build_hybrid_prompt(text, similar_samples, analysis)
            
            # è°ƒç”¨APIç”Ÿæˆç»“æœ
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"ç”Ÿæˆé”™è¯¯: {e}")
            return None

    def _analyze_with_local_model(self, text):
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆ†ææ–‡æœ¬ç‰¹å¾"""
        prompt = f"""åˆ†æä»¥ä¸‹æ–‡æœ¬çš„MBTIç›¸å…³ç‰¹å¾ï¼š
        
        æ–‡æœ¬: {text}
        
        è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        1. ç¤¾äº¤å€¾å‘ï¼ˆå¤–å‘E/å†…å‘Iï¼‰
        2. ä¿¡æ¯å¤„ç†æ–¹å¼ï¼ˆå®æ„ŸS/ç›´è§‰Nï¼‰ 
        3. å†³ç­–æ–¹å¼ï¼ˆæ€è€ƒT/æƒ…æ„ŸFï¼‰
        4. ç”Ÿæ´»æ–¹å¼ï¼ˆåˆ¤æ–­J/æ„ŸçŸ¥Pï¼‰
        
        å…³é”®ç‰¹å¾ï¼š"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(
            inputs.input_ids,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True
        )
        
        analysis = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return analysis.split("å…³é”®ç‰¹å¾ï¼š")[-1].strip()

    def _build_hybrid_prompt(self, text, similar_samples, analysis):
        """æ„å»ºæ··åˆæç¤ºæ¨¡æ¿"""
        examples = "\n".join([
            f"ç›¸ä¼¼æ–‡æœ¬ {i+1}: {sample[0]}\nå¯¹åº”MBTI: {sample[1]}"
            for i, sample in enumerate(similar_samples)
        ])
        
        return f"""è¯·ç»¼åˆä»¥ä¸‹ä¿¡æ¯é¢„æµ‹MBTIç±»å‹ï¼ˆåªéœ€è¿”å›4ä¸ªå¤§å†™å­—æ¯ï¼‰ï¼š
        
        ### å¾…åˆ†ææ–‡æœ¬ï¼š
        {text}
        
        ### æœ¬åœ°æ¨¡å‹åˆ†æç»“æœï¼š
        {analysis}
        
        ### ç›¸ä¼¼æ–‡æœ¬ç¤ºä¾‹ï¼š
        {examples}
        
        ### é¢„æµ‹é€»è¾‘ï¼š
        1. ç»“åˆæœ¬åœ°æ¨¡å‹æå–çš„å…³é”®ç‰¹å¾
        2. å‚è€ƒç›¸ä¼¼æ–‡æœ¬çš„MBTIåˆ†å¸ƒ
        3. ç‰¹åˆ«å…³æ³¨J/Pç»´åº¦çš„å€¾å‘æ€§
        
        æœ€ç»ˆMBTIç±»å‹ï¼š"""

def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—å‡½æ•°"""
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', ' ', text, flags=re.MULTILINE)
    text = re.sub(r'<.*?>', ' ', text)
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def evaluate(system, test_df):
    """å¢å¼ºè¯„ä¼°å‡½æ•°"""
    print("\nğŸ§ª å¼€å§‹æ··åˆè¯„ä¼°...")
    actual, predicted = [], []
    valid_types = {
        'INFJ', 'ENTP', 'INTP', 'INTJ', 'INFP', 
        'ESTP', 'ESFP', 'ISFJ', 'ISFP', 'ISTP',
        'ISTJ', 'ENFP', 'ENFJ', 'ESFJ', 'ESTJ', 'ENTJ'
    }
    
    # å­˜å‚¨é¢„æµ‹æ¦‚ç‡
    prob_dict = {t: [] for t in valid_types}
    
    with tqdm(total=len(test_df), desc="ğŸ” æ··åˆé¢„æµ‹") as pbar:
        for _, row in test_df.iterrows():
            pred = system.generate_with_api(row['posts'])
            if pred and pred in valid_types:
                actual.append(row['type'])
                predicted.append(pred)
                
                # === å…³é”®ä¿®æ”¹ï¼šåŸºäºæ£€ç´¢ç»“æœçš„åˆ†å¸ƒç”Ÿæˆæ¦‚ç‡ ===
                similar_samples = system.retrieve_enhanced(row['posts'], k=5)
                similar_types = [s[1] for s in similar_samples]
                
                # è®¡ç®—æ¯ä¸ªç±»å‹çš„å‡ºç°é¢‘ç‡ä½œä¸ºæ¦‚ç‡åŸºç¡€
                type_counts = {t: similar_types.count(t) for t in valid_types}
                total = sum(type_counts.values())
                
                # æ·»åŠ é¢„æµ‹ç±»å‹å¢å¼ºå’Œéšæœºå™ªå£°
                for t in valid_types:
                    if t == pred:
                        prob = (type_counts[t] + 2) / (total + 2)  # ç»™é¢„æµ‹ç±»å‹åŠ æƒ
                    else:
                        prob = type_counts[t] / (total + 2)
                    prob_dict[t].append(prob + 0.1*np.random.random())  # æ·»åŠ å™ªå£°
            pbar.update(1)
    
    # è®¡ç®—ç»´åº¦æŒ‡æ ‡
    dimensions = {
        'E/I': {'index': 0, 'pos': ['E'], 'neg': ['I']},
        'S/N': {'index': 1, 'pos': ['S'], 'neg': ['N']},
        'T/F': {'index': 2, 'pos': ['T'], 'neg': ['F']},
        'J/P': {'index': 3, 'pos': ['J'], 'neg': ['P']}
    }
    
    results = {}
    for dim, config in dimensions.items():
        # å‡†å¤‡äºŒåˆ†ç±»æ ‡ç­¾
        y_true = [1 if a[config['index']] in config['pos'] else 0 for a in actual]
        y_pred = [1 if p[config['index']] in config['pos'] else 0 for p in predicted]
        
        # è®¡ç®—æ¦‚ç‡ï¼ˆå°†ç›¸å…³ç±»å‹çš„æ¦‚ç‡ç›¸åŠ ï¼‰
        y_prob = []
        for i in range(len(actual)):
            prob = 0.0
            for t in valid_types:
                if t[config['index']] in config['pos']:
                    prob += prob_dict[t][i]
            y_prob.append(prob)
        
        # è®¡ç®—æŒ‡æ ‡
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        auc = roc_auc_score(y_true, y_prob) if len(set(y_true)) > 1 else 0.5
        
        results[dim] = {
            'ACC': acc,
            'F1': f1,
            'AUC': auc
        }
    
    # æ‰“å°ç¾è§‚ç»“æœ
    print("\nğŸ“Š æ··åˆè¯„ä¼°ç»“æœ")
    print("{:<8} {:<10} {:<10} {:<10}".format("ç»´åº¦", "ACC", "F1åˆ†æ•°", "AUC"))
    print("-"*38)
    for dim, scores in results.items():
        print("{:<8} {:<10.4f} {:<10.4f} {:<10.4f}".format(
            dim, scores['ACC'], scores['F1'], scores['AUC']))
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_acc = np.mean([v['ACC'] for v in results.values()])
    avg_f1 = np.mean([v['F1'] for v in results.values()])
    avg_auc = np.mean([v['AUC'] for v in results.values()])
    
    print("-"*38)
    print(f"å¹³å‡æŒ‡æ ‡: ACC={avg_acc:.4f} F1={avg_f1:.4f} AUC={avg_auc:.4f}")
    
    return results, (avg_acc, avg_f1, avg_auc)
    

if __name__ == "__main__":
    # åˆå§‹åŒ–æ··åˆç³»ç»Ÿ
    system = HybridMBTISystem(
        adapter_path=ADAPTER_PATH,
        encoder_path=ENCODER_PATH,
        vector_db_dir="./my_vector_db"  # è‡ªå®šä¹‰å­˜å‚¨ç›®å½•
    )
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_df = pd.read_csv('./mnt/train_0328.csv')
    test_df = pd.read_csv('./mnt/tt.csv')
    
    # æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“
    system.build_vector_db(train_df)  # é¦–æ¬¡è¿è¡Œä¼šæ„å»ºå¹¶ä¿å­˜ï¼Œåç»­è‡ªåŠ¨åŠ è½½
    
    # æ‰§è¡Œè¯„ä¼°ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
    #results, avg_f1 = evaluate(system, test_df)
    
    # ç¤ºä¾‹æ¼”ç¤º
    print("\nâœ¨ æ··åˆé¢„æµ‹ç¤ºä¾‹:")
    samples = [
        "I enjoy creating detailed schedules and sticking to plans.basically up to age 10, my favorite toys consisted of trucks, tractors, and electric trains. I then transitioned into real tractors, real trucks, firearms, knives, and other awesome things....This part of the site is unprotected.Yup, makes perfect sense because that's what I would have done..."
    ]
    for text in samples:
        print(f"\nğŸ“ è¾“å…¥æ–‡æœ¬: {text}")
        
        # å±•ç¤ºæ£€ç´¢ç»“æœ
        similar = system.retrieve_enhanced(text, k=5)  # ä½¿ç”¨top_kå‚æ•°
        print("\nğŸ” æ£€ç´¢åˆ°ç›¸ä¼¼æ–‡æœ¬:")
        for i, (sample, mbti) in enumerate(similar):
            print(f"{i+1}. {mbti}: {sample[:100]}...")
        
        # å±•ç¤ºæœ¬åœ°åˆ†æ
        analysis = system._analyze_with_local_model(text)
        print("\nğŸ§  æœ¬åœ°æ¨¡å‹åˆ†æ:")
        print(f"{analysis[:30]}...")
        
        # æœ€ç»ˆé¢„æµ‹
        pred = system.generate_with_api(text, top_k=5)
        print(f"\nğŸ¯ æœ€ç»ˆé¢„æµ‹MBTI: {pred}")
